---
title: LangChain.jså­¦ä¹ ç¬”è®°ï¼ˆ1ï¼‰
date: 2025-05-06 11:34:11
tags:
- LangChain.js
- LLM
- AI
categories: AI
---

LangChain.jså®˜æ–¹æ–‡æ¡£ï¼š<https://js.langchain.com/>

<!-- more -->

## éœ€è¦ç”¨åˆ°çš„ä¸€äº›åŒ…

`@langchain/community @langchain/core @langchain/openai dotenv langchain`

@langchain/community: ç”¨æ¥å¼•å…¥ä¸€äº›ç¤¾åŒºçš„åŒ…ï¼Œæ¯”å¦‚å„ç§å‘é‡åº“é€‚é…å™¨ã€å„ç§embeddingçš„æ”¯æŒã€å„ç§loader

@langchain/coreï¼šä¸»è¦æ˜¯å®˜æ–¹å®ç°çš„ä¸€äº›åº“

@langchain/openaiï¼šè°ƒç”¨openaiæˆ–è€…ç¬¬ä¸‰æ–¹å…¼å®¹openaiçš„æ¥å£

dotenvï¼šåŠ è½½ç¯å¢ƒå˜é‡

langchainï¼šå¥½åƒæœ‰äº›å·¥å…·å³ä¸åœ¨@langchain/communityä¹Ÿä¸åœ¨@langchain/coreï¼Œåªæœ‰langchainåŒ…æœ‰ï¼Œå¾ˆå¥‡æ€ª

## å¼€å§‹

### è·å–å¯¹è¯æœåŠ¡

è°ƒç”¨ollamaï¼š

```ts
import { Ollama } from "@langchain/community/llms/ollama";

const ollama = new Ollama({
  baseUrl: "http://localhost:11434", 
  model: "llama2", 
});


const res = await ollama.invoke("è®²ä¸ªç¬‘è¯")
```

è°ƒç”¨ç¬¬ä¸‰æ–¹æ¥å£ï¼Œæ–°å»º.envæ–‡ä»¶

```plaintext .env
OPENAI_API_KEY=xxx
```

è°ƒç”¨ç«å±±

```ts
import "dotenv/config";
import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage } from "@langchain/core/messages";


const chatModel = new ChatOpenAI({
    configuration: {
        baseURL: "https://ark.cn-beijing.volces.com/api/v3",
    },
    model: 'deepseek-v3-250324'
});

console.log(await chatModel.invoke([
    new HumanMessage("Tell me a joke")
]));
```

### LCEL

LCELï¼ˆLangChain Expression Languageï¼‰

ä¸€æ¡ Chain ç»„æˆçš„æ¯ä¸ªæ¨¡å—éƒ½æ˜¯ç»§æ‰¿è‡ª Runnable è¿™ä¸ªæ¥å£ï¼Œè€Œä¸€æ¡ Chain ä¹Ÿæ˜¯ç»§æ‰¿è‡ªè¿™ä¸ªæ¥å£ï¼Œæ‰€ä»¥ä¸€æ¡ Chain ä¹Ÿå¯ä»¥å¾ˆè‡ªç„¶çš„æˆä¸ºå¦ä¸€ä¸ª Chain çš„ä¸€ä¸ªæ¨¡å—ã€‚å¹¶ä¸”æ‰€æœ‰ Runnable éƒ½æœ‰ç›¸åŒçš„è°ƒç”¨æ–¹å¼ã€‚ æ‰€ä»¥åœ¨æˆ‘ä»¬å†™ Chain çš„æ—¶å€™å°±å¯ä»¥è‡ªç”±ç»„åˆå¤šä¸ª Runnable çš„æ¨¡å—æ¥å½¢æˆå¤æ‚çš„ Chainã€‚

å¯¹äºä»»æ„ Runnable å¯¹è±¡ï¼Œå…¶éƒ½ä¼šæœ‰è¿™å‡ ä¸ªå¸¸ç”¨çš„æ ‡å‡†çš„è°ƒç”¨æ¥å£ï¼š

- invoke åŸºç¡€çš„è°ƒç”¨ï¼Œå¹¶ä¼ å…¥å‚æ•°
- batch æ‰¹é‡è°ƒç”¨ï¼Œè¾“å…¥ä¸€ç»„å‚æ•°
- stream è°ƒç”¨ï¼Œå¹¶ä»¥ stream æµçš„æ–¹å¼è¿”å›æ•°æ®
- streamLog é™¤äº†åƒ stream æµä¸€æ ·è¿”å›æ•°æ®ï¼Œå¹¶ä¼šè¿”å›ä¸­é—´çš„è¿è¡Œç»“æœ

#### invoke

é¦–å…ˆï¼Œæˆ‘ä»¬ç”¨æœ€åŸºç¡€çš„ ChatOpenAIï¼Œè¿™æ˜¾ç„¶æ˜¯ä¸€ä¸ª Runnable å¯¹è±¡ï¼Œæˆ‘ä»¬ä»¥æ­¤ä¸ºä¾‹æ¥è®©å¤§å®¶ç†Ÿæ‚‰ LCEL ä¸­ Runnable ä¸­å¸¸è§çš„è°ƒç”¨æ¥å£ã€‚ å…¶ä¸­ HumanMessage ä½ å¯ä»¥ç†è§£æˆæ„å»ºä¸€ä¸ªç”¨æˆ·è¾“å…¥ï¼Œå„ç§ Message çš„ä»‹ç»æˆ‘ä»¬ä¼šåœ¨åç»­ç« èŠ‚ä¸­å±•å¼€ä»‹ç»ã€‚ æ³¨æ„è¿™é‡Œ chatModel éœ€è¦çš„è¾“å…¥æ˜¯ä¸€ä¸ª Message çš„åˆ—è¡¨ã€‚

```js
import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage } from "@langchain/core/messages";

const model = new ChatOpenAI();

await model.invoke([
    new HumanMessage("Tell me a joke")
])
```

è¾“å‡ºï¼š

```json
AIMessage {
  "id": "021746503328980713197eb3e0d8a6a0914d987ddc72be16a86c7",        
  "content": "Sure! Here's a classic for you:  \n\n**Why donâ€™t skeletons fight each other?**  \n*Because they donâ€™t have the guts!*  \n\nWant another? ğŸ˜„",
  "additional_kwargs": {},
  "response_metadata": {
    "tokenUsage": {
      "promptTokens": 7,
      "completionTokens": 39,
      "totalTokens": 46
    },
    "finish_reason": "stop",
    "model_name": "deepseek-v3-250324"
  },
  "tool_calls": [],
  "invalid_tool_calls": [],
  "usage_metadata": {
    "output_tokens": 39,
    "input_tokens": 7,
    "total_tokens": 46,
    "input_token_details": {
      "cache_read": 0
    },
    "output_token_details": {
      "reasoning": 0
    }
  }
}
```

#### batch

```js
await simpleChain.batch([
    [ new HumanMessage("Tell me a joke") ],
    [ new HumanMessage("Hi, Who are you?") ],
])
```

```json
[
  "Why don't scientists trust atoms?\n\nBecause they make up everything!",
  "Hello! I'm OpenAI, or more specifically an artificial intelligence programmed to help answer questio"... 89 more characters
]
```

#### stream

```js
const stream = await simpleChain.stream([
     new HumanMessage("Tell me a joke")
])

for await (const chunk of stream){
    console.log(chunk)
}
```

streamLog çš„ä½¿ç”¨è¾ƒå°‘ï¼Œä»–ä¼šåœ¨æ¯æ¬¡è¿”å› chunk çš„æ—¶å€™ï¼Œè¿”å›å®Œæ•´çš„å¯¹è±¡ï¼Œæˆ‘ä»¬ä¸æ·±å…¥ä»‹ç»ï¼Œæ„Ÿå…´è¶£çš„å¯ä»¥è¿è¡Œä¸‹è¿°ä»£ç è§‚å¯Ÿå…¶æ¯ä¸ª chunk çš„è¿”å›å€¼ï¼Œå¹¶æ ¹æ®è‡ªå·±éœ€è¦å»ä½¿ç”¨ã€‚

#### fallback

withFallbacks æ˜¯ä»»ä½• runnable éƒ½æœ‰çš„ä¸€ä¸ªå‡½æ•°ï¼Œå¯ä»¥ç»™å½“å‰ runnable å¯¹è±¡æ·»åŠ  fallback ç„¶åç”Ÿæˆä¸€ä¸ªå¸¦ fallback çš„ RunnableWithFallbacks å¯¹è±¡ï¼Œè¿™é€‚åˆæˆ‘ä»¬å°†è‡ªå·±çš„ fallback é€»è¾‘å¢åŠ åˆ° LCEL ä¸­ã€‚

ä¾‹å¦‚ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªä¸€å®šä¼šå¤±è´¥çš„ llm ï¼š

```hs
import { ChatOpenAI } from "@langchain/openai";

const fakeLLM = new ChatOpenAI({
    azureOpenAIApiKey: "123",
    maxRetries: 0,
});

await fakeLLM.invoke("ä½ å¥½")
```

å› ä¸ºå¤§å¤š runnable éƒ½è‡ªå¸¦å‡ºé”™é‡è¯•çš„æœºåˆ¶ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨è¿™å°†é‡è¯•çš„æ¬¡æ•° maxRetries è®¾ç½®ä¸º 0ã€‚

ç„¶åï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå¯ä»¥æˆåŠŸçš„ llmï¼Œå¹¶è®¾ç½®ä¸º fallbackï¼š

```js
const realLLM = new ChatOpenAI()
const llmWithFallback = fakeLLM.withFallbacks({
    fallbacks: [realLLM]
})

await llmWithFallback.invoke("ä½ å¥½")
```

å°±ä¼šè¾“å‡ºæ­£ç¡®çš„ç»“æœã€‚

## Prompt

é¦–å…ˆæˆ‘ä»¬å­¦ä¹ åŸºç¡€çš„ PromptTemplate æ¥ç†è§£ langchain ä¸­æ˜¯å¦‚ä½•æ„å»ºå’Œç®¡ç† prompt templateã€‚

PromptTemplate æ˜¯å¸®åŠ©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªåŒ…å«å˜é‡çš„å­—ç¬¦ä¸²æ¨¡ç‰ˆï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å‘è¯¥ç±»çš„å¯¹è±¡è¾“å…¥ä¸åŒçš„å˜é‡å€¼æ¥ç”Ÿæˆæ¨¡ç‰ˆæ¸²æŸ“çš„ç»“æœã€‚ è¿™å¯ä»¥æ–¹ä¾¿çš„è®©æˆ‘ä»¬å®šä¹‰ä¸€ç»„ prompt æ¨¡æ¿ï¼Œç„¶ååœ¨è¿è¡Œæ—¶æ ¹æ®ç”¨æˆ·çš„è¾“å…¥åŠ¨æ€åœ°å¡«å……å˜é‡ä»è€Œç”Ÿæˆ promptã€‚

### æ— å˜é‡ template

```js
import { PromptTemplate } from "@langchain/core/prompts";

const greetingPrompt = new PromptTemplate({
  inputVariables: [],
  template: "hello world",
});
const formattedGreetingPrompt = await greetingPrompt.format();

console.log(formattedGreetingPrompt);
```

PromptTemplate å°±æ˜¯æœ€åŸºç¡€çš„ templateï¼Œæˆ‘ä»¬ä¸ä¼ å…¥ä»»ä½•å˜é‡ï¼ˆinputVariables: []ï¼‰ï¼Œè¿™è·Ÿç¡¬ç¼–ç ä¸€ä¸ªå­—ç¬¦ä¸²æ²¡ä»»ä½•åŒºåˆ«ã€‚ è°ƒç”¨ prompt template çš„æ–¹å¼å°±æ˜¯ formatï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰ä»»ä½•å˜é‡ï¼Œä¹Ÿå°±æ²¡æœ‰ä»»ä½•å‚æ•°ã€‚

æ²¡æœ‰å˜é‡çš„ prompt template ä½¿ç”¨çš„å¾ˆå°‘ï¼Œè¿™é‡Œä¸»è¦ä»¥æ­¤å¸®åŠ©å¤§å®¶ç†è§£ template çš„æ¦‚å¿µã€‚

### å«å˜é‡çš„ template

```js
const personalizedGreetingPrompt = new PromptTemplate({
  inputVariables: ["name"],
  template: "helloï¼Œ{name}",
});
const formattedPersonalizedGreeting = await personalizedGreetingPrompt.format({
  name: "Kai",
});

console.log(formattedPersonalizedGreeting);
// helloï¼ŒKai
```

å…¶ API æ¯”è¾ƒå®¹æ˜“ç†è§£ï¼Œä½¿ç”¨ {} æ¥åŒ…è£¹ä½å˜é‡ï¼Œç„¶ååœ¨ inputVariables å£°æ˜ç”¨åˆ°çš„å˜é‡åç§°ã€‚å› ä¸ºæœ‰äº†å˜é‡ï¼Œæ‰€ä»¥åœ¨è°ƒç”¨ format() å°±éœ€è¦ä¼ å…¥å¯¹åº”çš„å˜é‡ã€‚

åŒæ ·çš„å¤šå˜é‡çš„ template ä¹Ÿæ˜¯ç±»ä¼¼çš„

```js
const multiVariableGreetingPrompt = new PromptTemplate({
  inputVariables: ["timeOfDay", "name"],
  template: "good {timeOfDay}, {name}",
});
const formattedMultiVariableGreeting = await multiVariableGreetingPrompt.format({
  timeOfDay: "morning",
  name: "Kai",
});

console.log(formattedMultiVariableGreeting);
// good morning, Kai
```

å”¯ä¸€éœ€è¦æ³¨æ„çš„å°±æ˜¯ï¼Œå¦‚æœä½ çš„ prompt éœ€è¦ `{}`ï¼Œå¯ä»¥è¿™ä¹ˆè½¬ä¹‰ `{{}}`

```js
const multiVariableGreetingPrompt = new PromptTemplate({
  inputVariables: ["timeOfDay", "name"],
  template: "good {timeOfDay}, {name} {{test}}",
});
const formattedMultiVariableGreeting = await multiVariableGreetingPrompt.format({
  timeOfDay: "morning",
  name: "Kai",
});

console.log(formattedMultiVariableGreeting);
// good morning, Kai {test}
```

è¿™ä¹ˆåˆ›å»º template æœ‰ç‚¹ç¹çï¼Œ langchain ä¹Ÿæä¾›äº†ç®€ä¾¿çš„åˆ›å»ºæ–¹å¼

```js
const autoInferTemplate = PromptTemplate.fromTemplate("good {timeOfDay}, {name}");
console.log(autoInferTemplate.inputVariables);
// ['timeOfDay', 'name']

const formattedAutoInferTemplate = await autoInferTemplate.format({
  timeOfDay: "morning",
  name: "Kai",
});
console.log(formattedAutoInferTemplate)
// good morning, Kai
```

è¿™æ ·åˆ›å»º prompt çš„æ—¶å€™ï¼Œä¼šè‡ªåŠ¨ä»å­—ç¬¦ä¸²ä¸­æ¨æµ‹å‡ºéœ€è¦è¾“å…¥çš„å˜é‡ã€‚

### ä½¿ç”¨éƒ¨åˆ†å‚æ•°åˆ›å»º template

```js
const initialPrompt = new PromptTemplate({
  template: "è¿™æ˜¯ä¸€ä¸ª{type}ï¼Œå®ƒæ˜¯{item}ã€‚",
  inputVariables: ["type", "item"],
});


const partialedPrompt = await initialPrompt.partial({
  type: "å·¥å…·",
});

const formattedPrompt = await partialedPrompt.format({
  item: "é”¤å­",
});

console.log(formattedPrompt);
// è¿™æ˜¯ä¸€ä¸ªå·¥å…·ï¼Œå®ƒæ˜¯é”¤å­ã€‚

const formattedPrompt2 = await partialedPrompt.format({
  item: "æ”¹é”¥",
});

console.log(formattedPrompt2)
// è¿™æ˜¯ä¸€ä¸ªå·¥å…·ï¼Œå®ƒæ˜¯æ”¹é”¥ã€‚
```

### ä½¿ç”¨åŠ¨æ€å¡«å……å‚æ•°

å½“æˆ‘ä»¬éœ€è¦ï¼Œä¸€ä¸ª prompt template è¢« `format` æ—¶ï¼Œå®æ—¶åœ°åŠ¨æ€ç”Ÿæˆå‚æ•°æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‡½æ•°æ¥å¯¹ template éƒ¨åˆ†å‚æ•°è¿›è¡ŒæŒ‡å®šã€‚

```js
const getCurrentDateStr = () => {
  return new Date().toLocaleDateString();
};

const promptWithDate = new PromptTemplate({
  template: "ä»Šå¤©æ˜¯{date}ï¼Œ{activity}ã€‚",
  inputVariables: ["date", "activity"],
});

const partialedPromptWithDate = await promptWithDate.partial({
  date: getCurrentDateStr,
});

const formattedPromptWithDate = await partialedPromptWithDate.format({
  activity: "æˆ‘ä»¬å»çˆ¬å±±",
});

console.log(formattedPromptWithDate);
// è¾“å‡º: ä»Šå¤©æ˜¯2023/7/13ï¼Œæˆ‘ä»¬å»çˆ¬å±±ã€‚
```

æ³¨æ„ï¼Œå‡½æ•° `getCurrentDateStr` æ˜¯åœ¨ `format` è¢«è°ƒç”¨çš„æ—¶å€™å®æ—¶è¿è¡Œçš„ï¼Œä¹Ÿå°±æ˜¯å¯ä»¥åœ¨è¢«æ¸²æŸ“æˆå­—ç¬¦ä¸²æ—¶è·å–åˆ°æœ€æ–°çš„å¤–éƒ¨ä¿¡æ¯ã€‚ ç›®å‰è¿™é‡Œä¸æ”¯æŒä¼ å…¥å‚æ•°ï¼Œå¦‚æœéœ€è¦å‚æ•°ï¼Œå¯ä»¥ç”¨ js çš„é—­åŒ…è¿›è¡Œå‚æ•°çš„ä¼ é€’ã€‚
å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæ ¹æ®æ—¶é—´æ®µï¼ˆmorning, afternoon, eveningï¼‰è¿”å›ä¸åŒé—®å€™è¯­ï¼Œå¹¶ä¸”éœ€è¦å¸¦ä¸Šå½“å‰æ—¶é—´çš„éœ€æ±‚

```js
const getCurrentDateStr = () => {
  return new Date().toLocaleDateString();
};

function generateGreeting(timeOfDay) {
  return () => {
    const date = getCurrentDateStr()
    switch (timeOfDay) {
      case 'morning':
        return date + ' æ—©ä¸Šå¥½';
      case 'afternoon':
        return date + ' ä¸‹åˆå¥½';
      case 'evening':
        return date + ' æ™šä¸Šå¥½';
      default:
        return date + ' ä½ å¥½';
    }
  };
}

const prompt = new PromptTemplate({
  template: "{greeting}!",
  inputVariables: ["greeting"],
});

const currentTimeOfDay = 'afternoon';
const partialPrompt = await prompt.partial({
  greeting: generateGreeting(currentTimeOfDay),
});

const formattedPrompt = await partialPrompt.format();

console.log(formattedPrompt);
// è¾“å‡º: 3/21/2024 ä¸‹åˆå¥½!
```

### chat prompt

ä¸ºäº†æ–¹ä¾¿åœ°æ„å»ºå’Œå¤„ç†è¿™ç§ç»“æ„åŒ–çš„èŠå¤©æ¶ˆæ¯ï¼ŒLangChain æä¾›äº†å‡ ç§ä¸èŠå¤©ç›¸å…³çš„æç¤ºæ¨¡æ¿ç±»ï¼Œå¦‚ `ChatPromptTemplate`ã€`SystemMessagePromptTemplate`ã€`AIMessagePromptTemplate` å’Œ `HumanMessagePromptTemplate`ã€‚

å…¶ä¸­åé¢ä¸‰ä¸ªåˆ†åˆ«å¯¹åº”äº†ä¸€æ®µ ChatMessage ä¸åŒçš„è§’è‰²ã€‚åœ¨ OpenAI çš„å®šä¹‰ä¸­ï¼Œæ¯ä¸€æ¡æ¶ˆæ¯éƒ½éœ€è¦è·Ÿä¸€ä¸ª role å…³è”ï¼Œæ ‡è¯†æ¶ˆæ¯çš„å‘é€è€…ã€‚è§’è‰²çš„æ¦‚å¿µå¯¹ LLM ç†è§£å’Œæ„å»ºæ•´ä¸ªå¯¹è¯æµç¨‹éå¸¸é‡è¦ï¼Œç›¸åŒçš„å†…å®¹ç”±ä¸åŒçš„ role å‘é€å‡ºæ¥çš„æ„ä¹‰æ˜¯ä¸åŒçš„ã€‚

- `system` è§’è‰²çš„æ¶ˆæ¯é€šå¸¸ç”¨äºè®¾ç½®å¯¹è¯çš„ä¸Šä¸‹æ–‡æˆ–æŒ‡å®šæ¨¡å‹é‡‡å–ç‰¹å®šçš„è¡Œä¸ºæ¨¡å¼ã€‚è¿™äº›æ¶ˆæ¯ä¸ä¼šç›´æ¥æ˜¾ç¤ºåœ¨å¯¹è¯ä¸­ï¼Œä½†å®ƒä»¬å¯¹æ¨¡å‹çš„è¡Œä¸ºæœ‰æŒ‡å¯¼ä½œç”¨ã€‚ å¯ä»¥ç†è§£æˆæ¨¡å‹çš„å…ƒä¿¡æ¯ï¼Œæƒé‡éå¸¸é«˜ï¼Œåœ¨è¿™é‡Œæœ‰æ•ˆçš„æ„å»º prompt èƒ½å–å¾—éå¸¸å¥½çš„æ•ˆæœã€‚
- `user` è§’è‰²ä»£è¡¨çœŸå®ç”¨æˆ·åœ¨å¯¹è¯ä¸­çš„å‘è¨€ã€‚è¿™äº›æ¶ˆæ¯é€šå¸¸æ˜¯é—®é¢˜ã€æŒ‡ä»¤æˆ–è€…è¯„è®ºï¼Œåæ˜ äº†ç”¨æˆ·çš„æ„å›¾å’Œéœ€æ±‚ã€‚
- `assistant` è§’è‰²çš„æ¶ˆæ¯ä»£è¡¨AIæ¨¡å‹çš„å›å¤ã€‚è¿™äº›æ¶ˆæ¯æ˜¯æ¨¡å‹æ ¹æ®systemçš„æŒ‡ç¤ºå’Œuserçš„è¾“å…¥ç”Ÿæˆçš„ã€‚

æˆ‘ä»¬ä»¥ä¸€ä¸ªåŸºç¡€çš„ç¿»è¯‘ chatbot æ¥è®²è§£è¿™å‡ ä¸ªå¸¸è§ chat templateï¼Œæˆ‘ä»¬å…ˆæ„å»ºä¸€ä¸ª system message æ¥ç»™ llm æŒ‡å®šæ ¸å¿ƒçš„å‡†åˆ™

```js
import { SystemMessagePromptTemplate } from "@langchain/core/prompts";

const translateInstructionTemplate = SystemMessagePromptTemplate.fromTemplate(`ä½ æ˜¯ä¸€ä¸ªä¸“
ä¸šçš„ç¿»è¯‘å‘˜ï¼Œä½ çš„ä»»åŠ¡æ˜¯å°†æ–‡æœ¬ä»{source_lang}ç¿»è¯‘æˆ{target_lang}ã€‚`);
```

ç„¶åæ„å»ºä¸€ä¸ªç”¨æˆ·è¾“å…¥çš„ä¿¡æ¯

```js
import { HumanMessagePromptTemplate } from "@langchain/core/prompts";

const userQuestionTemplate = HumanMessagePromptTemplate.fromTemplate("è¯·ç¿»è¯‘è¿™å¥è¯ï¼š{text}")
```

ç„¶åå°†è¿™ä¸¤ä¸ªä¿¡æ¯ç»„åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå¯¹è¯ä¿¡æ¯

```js
import { ChatPromptTemplate } from "@langchain/core/prompts";

const chatPrompt = ChatPromptTemplate.fromMessages([
  translateInstructionTemplate,
  userQuestionTemplate,
]);
```

ç„¶åæˆ‘ä»¬å°±å¯ä»¥ç”¨ä¸€ä¸ª `fromMessages` æ¥æ ¼å¼åŒ–æ•´ä¸ªå¯¹è¯ä¿¡æ¯

```js
const formattedChatPrompt = await chatPrompt.formatMessages({
  source_lang: "ä¸­æ–‡",
  target_lang: "æ³•è¯­",
  text: "ä½ å¥½ï¼Œä¸–ç•Œ",
});

console.log(formattedChatPrompt)
```

```json
[
  SystemMessage {
    lc_serializable: true,
    lc_kwargs: {
      content: "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘å‘˜ï¼Œä½ çš„ä»»åŠ¡æ˜¯å°†æ–‡æœ¬ä»ä¸­æ–‡ç¿»è¯‘æˆæ³•è¯­ã€‚",
      additional_kwargs: {},
      response_metadata: {}
    },
    lc_namespace: [ "langchain_core", "messages" ],
    content: "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘å‘˜ï¼Œä½ çš„ä»»åŠ¡æ˜¯å°†æ–‡æœ¬ä»ä¸­æ–‡ç¿»è¯‘æˆæ³•è¯­ã€‚",
    name: undefined,
    additional_kwargs: {},
    response_metadata: {}
  },
  HumanMessage {
    lc_serializable: true,
    lc_kwargs: {
      content: "è¯·ç¿»è¯‘è¿™å¥è¯ï¼šä½ å¥½ï¼Œä¸–ç•Œ",
      additional_kwargs: {},
      response_metadata: {}
    },
    lc_namespace: [ "langchain_core", "messages" ],
    content: "è¯·ç¿»è¯‘è¿™å¥è¯ï¼šä½ å¥½ï¼Œä¸–ç•Œ",
    name: undefined,
    additional_kwargs: {},
    response_metadata: {}
  }
]
```